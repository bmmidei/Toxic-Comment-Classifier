{
  "cells": [
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "48e7d0b17674d65f7ca1d5994325e7ffe485af69"
      },
      "cell_type": "markdown",
      "source": "# LSTM Training\nThis notebook can be used to train an LSTM for text classification and generate predictions for the kaggle competition found [here](https://www.kaggle.com/c/quora-insincere-questions-classification). \n\nThe notebook utilizes Keras and GloVe for preprocessing using word embeddings. Then, Keras with Tensorflow backend is used for training a deep LSTM. \n\nEnsure that the train.csv and test.csv are in the data/ directory of this project. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "31651e5a58fc9229e683afac2cbc7b0948a5f240"
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport os\n\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Dense, Embedding, CuDNNLSTM, Bidirectional, SpatialDropout1D, GlobalMaxPool1D, Dropout\nfrom keras.models import Model\n\n%load_ext autoreload\n%autoreload 2",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9dc94191884a37933e64d2d822040a9779a621fd"
      },
      "cell_type": "code",
      "source": "# Load in training and testing data\ntrain_df = pd.read_csv('../input/train.csv')\ntrain_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1e5c03f5d075dfe8f48ce5edd696c380804e01fa"
      },
      "cell_type": "code",
      "source": "print('The average word length of questions in the training set is {0:.0f}.'\\\n          .format(np.mean(train_df['question_text'].apply(lambda x: len(x.split())))))\nprint('The maximum word length for a question in the training set is {0:.0f}.'\\\n          .format(np.max(train_df['question_text'].apply(lambda x: len(x.split())))))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "78488b53255b19d912481adbc91f5bcf0410e689"
      },
      "cell_type": "code",
      "source": "# Extract the training data and corresponding labels\ntext = train_df['question_text'].fillna('unk').values\nlabels = train_df['target'].values\n\n# Split into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(text, labels,\\\n                                                  test_size=0.2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e00b006b3a940240da8d4d8b2bfda5765a9f7a40"
      },
      "cell_type": "code",
      "source": "embed_size = 300 # Size of each word vector\nmax_words = 50000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 100 # max number of words in a question to use",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3d68827dba2948ee2ef7dd4be10a1fa4dd629e4a"
      },
      "cell_type": "code",
      "source": "## Tokenize the sentences\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(list(X_train))\n\nX_train = tokenizer.texts_to_sequences(X_train)\nX_val = tokenizer.texts_to_sequences(X_val)\n\nword_index = tokenizer.word_index\nprint('The word index consists of {} unique tokens.'.format(len(word_index)))\n\n## Pad the sentences \nX_train = pad_sequences(X_train, maxlen=maxlen)\nX_val = pad_sequences(X_val, maxlen=maxlen)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "55461170dae0671d226504bd23aec3fa43f31cb3"
      },
      "cell_type": "code",
      "source": "# Create the embedding dictionary from the word embedding file\nembedding_dict = {}\nfilename = os.path.join('../input/embeddings/', 'glove.840B.300d/glove.840B.300d.txt')\nwith open(filename) as f:\n    for line in f:\n        line = line.split()\n        token = line[0]\n        try:\n            coefs = np.asarray(line[1:], dtype='float32')\n            embedding_dict[token] = coefs\n        except:\n            pass\nprint('The embedding dictionary has {} items'.format(len(embedding_dict)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0fc9a47b228f234e5162d596091b72a69e8c5a7f"
      },
      "cell_type": "code",
      "source": "# Create the embedding layer weight matrix\nembed_mat = np.zeros(shape=[max_words, embed_size])\nfor word, idx in word_index.items():\n    # Word index is ordered from most frequent to least frequent\n    # Ignore words that occur less frequently\n    if idx >= max_words: continue\n    vector = embedding_dict.get(word)\n    if vector is not None:\n        embed_mat[idx] = vector",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1e4ec3f078dc5ca43669238813d5ebcadf6ab39d"
      },
      "cell_type": "code",
      "source": "def create_lstm():\n    input = Input(shape=(maxlen,))\n    x = Embedding(max_words, embed_size, weights=[embed_mat], trainable=False)(input)\n    x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n    x = GlobalMaxPool1D()(x)\n    x = Dense(16, activation=\"relu\")(x)\n    x = Dropout(0.1)(x)\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=input, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    print(model.summary())\n    \n    return model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "738691c29ce46a05923efc18e3fada0c308e611e"
      },
      "cell_type": "code",
      "source": "lstm = create_lstm()\nlstm.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=3, batch_size=512)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "61aeb24e48c578594d26d0631a46aad49a215c4e"
      },
      "cell_type": "markdown",
      "source": "# Predictions\nThe remainder of this notebok will generate predictions from the test set and write them to a submission csv file. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "91e9da1a1d551932c34405c3a7ed185632f12260"
      },
      "cell_type": "code",
      "source": "test_df = pd.read_csv('../input/test.csv')\nX_test = test_df['question_text'].values\n\nX_test = tokenizer.texts_to_sequences(X_test)\nX_test = pad_sequences(X_test, maxlen=maxlen)\n\npreds = np.rint(lstm.predict([X_test], batch_size=1024, verbose=1))\ntest_df['prediction'] = preds\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5b7bd3f417dfa01973f20f12fe967ec61ff3dc69"
      },
      "cell_type": "code",
      "source": "test_df.loc[test_df['prediction'] == 1]['question_text'].head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "988fa1a03853289b65bb4c1845c5b7c333868cb8"
      },
      "cell_type": "code",
      "source": "test_df = test_df.drop('question_text', axis=1)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cd0c4aaecad334f806445e57e32012d830d938fd"
      },
      "cell_type": "code",
      "source": "test_df.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "0ec35902ec2e447646ee346034bc1a8671586856"
      },
      "cell_type": "code",
      "source": "test_df.to_csv('submission.csv', index=False)\n",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}